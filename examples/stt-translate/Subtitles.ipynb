{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP_6cflBTZEa"
      },
      "source": [
        "# Audio Processing and Transcription using Sarvam AI API and VAD\n",
        "\n",
        "This notebook demonstrates how to process an audio file, detect speech segments using Voice Activity Detection (VAD), and transcribe those segments using the Sarvam AI Speech-to-Text API. The results are then saved in an SRT file format.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have the following installed:\n",
        "\n",
        "- Python 3.7 or higher\n",
        "- Required Python packages: `torch`, `numpy`, `librosa`, `soundfile`, `tqdm`, `pydub`, `requests`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NrPAhK2LVuO",
        "outputId": "0d1c6c3d-0846-430e-a80c-806332e0ef93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy librosa soundfile tqdm pydub requests torchaudio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import required packages\n",
        "\n",
        "To use the required libraries in your Python script, you can import them in a single line as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\Desktop\\sarvam-ai-cookbook\\.venv\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
            "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
          ]
        }
      ],
      "source": [
        "import torch, torchaudio\n",
        "import io\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from pydub import AudioSegment\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13qJ0ks7Wp3P"
      },
      "source": [
        "### **Set Up the API Endpoint and Payload**\n",
        "\n",
        "To use the Saaras API, you need an API subscription key. Follow these steps to set up your API key:\n",
        "\n",
        "1. **Obtain your API key**: If you donâ€™t have an API key, sign up on the [Sarvam AI Dashboard](https://dashboard.sarvam.ai/) to get one.\n",
        "2. **Replace the placeholder key**: In the code below, replace \"YOUR_SARVAM_AI_API_KEY\" with your actual API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpoUThmxWuOd"
      },
      "outputs": [],
      "source": [
        "SARVAM_AI_API=\"YOUR_SARVAM_AI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5sCN2dZTya-"
      },
      "source": [
        "## **Configuration**\n",
        "\n",
        "Set up the configuration parameters for the audio processing and VAD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IT0ExtXmUCcf"
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000  # Set the sample rate for loading audio\n",
        "vad_threshold = 0.5  # Threshold for VAD\n",
        "combine_duration = 8  # Maximum duration for combined segments\n",
        "combine_gap = 1  # Maximum gap between segments to combine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX1h5b-CUcZx"
      },
      "source": [
        "### Load VAD Model\n",
        "\n",
        "We use the Silero VAD model from the `torch.hub` to detect speech segments in the audio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UqKjAApNUf4L"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def get_vad_probs(model, audio, sample_rate=16000):\n",
        "    audio = torch.as_tensor(audio, dtype=torch.float32)\n",
        "    window_size_samples = 512 if sample_rate == 16000 else 256\n",
        "\n",
        "    model.reset_states()\n",
        "    audio_length_samples = len(audio)\n",
        "\n",
        "    speech_probs = []\n",
        "    for current_start_sample in range(0, audio_length_samples, window_size_samples):\n",
        "        chunk = audio[current_start_sample: current_start_sample + window_size_samples]\n",
        "        if len(chunk) < window_size_samples:\n",
        "            chunk = torch.nn.functional.pad(chunk, (0, int(window_size_samples - len(chunk))))\n",
        "        speech_prob = model(chunk, sample_rate).item()\n",
        "        speech_probs.append(speech_prob)\n",
        "\n",
        "    return speech_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leYXyGE8Uh7p"
      },
      "source": [
        "### Extract Utterances\n",
        "\n",
        "This function extracts the start and end times of speech segments based on the VAD probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ApMimO20UjPL"
      },
      "outputs": [],
      "source": [
        "def get_utterances(vad_probs, threshold=0.5, frame_duration=0.032):\n",
        "    \"\"\"Extracts utterances (start and end times) based on VAD probabilities.\"\"\"\n",
        "    utterances = []\n",
        "    in_utterance = False\n",
        "    utterance_start = 0\n",
        "\n",
        "    for i, prob in enumerate(vad_probs):\n",
        "        if prob > threshold and not in_utterance:\n",
        "            in_utterance = True\n",
        "            utterance_start = i * frame_duration\n",
        "        elif prob <= threshold and in_utterance:\n",
        "            in_utterance = False\n",
        "            utterance_end = i * frame_duration\n",
        "            if utterance_end - utterance_start > 0:\n",
        "                utterances.append((utterance_start, utterance_end))\n",
        "\n",
        "    if in_utterance:\n",
        "        utterances.append((utterance_start, len(vad_probs) * frame_duration))\n",
        "\n",
        "    return utterances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0-xhhfUk-q"
      },
      "source": [
        "### Merge Segments\n",
        "\n",
        "This function merges segments that are close to each other and within the specified duration limit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C_nQOGe3Ul-7"
      },
      "outputs": [],
      "source": [
        "def merge_segments(segments, max_duration=8, max_gap=1):\n",
        "    \"\"\"Combines segments with pauses shorter than `max_gap` seconds, with total duration limit.\"\"\"\n",
        "    merged_segments = []\n",
        "    if not segments:\n",
        "        return merged_segments  # Return empty if no segments are found\n",
        "\n",
        "    current_start, current_end = segments[0]\n",
        "\n",
        "    for start, end in segments[1:]:\n",
        "        combined_duration = (end - current_start)\n",
        "\n",
        "        if (start - current_end <= max_gap) and (combined_duration <= max_duration):\n",
        "            current_end = end\n",
        "        else:\n",
        "            merged_segments.append((current_start, current_end))\n",
        "            current_start, current_end = start, end\n",
        "\n",
        "    merged_segments.append((current_start, current_end))\n",
        "    return merged_segments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDafi2v1UnWl"
      },
      "source": [
        "### Process Audio\n",
        "\n",
        "This function processes the audio file to detect speech segments using the VAD model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uZImMlOCUokQ"
      },
      "outputs": [],
      "source": [
        "def process_audio(audio_file):\n",
        "    vad_model, _ = torch.hub.load(\n",
        "        repo_or_dir='snakers4/silero-vad',\n",
        "        model='silero_vad',\n",
        "        force_reload=False,\n",
        "        onnx=False\n",
        "    )\n",
        "    vad_model.eval()\n",
        "\n",
        "    audio, _ = librosa.load(audio_file, sr=sample_rate)\n",
        "    speech_probs = get_vad_probs(vad_model, audio, sample_rate)\n",
        "    utterances = get_utterances(speech_probs, threshold=vad_threshold)\n",
        "\n",
        "    if not utterances:\n",
        "        print(f\"No VAD regions detected for {audio_file}.\")\n",
        "        return\n",
        "    merged_segments = merge_segments(utterances, max_duration=combine_duration, max_gap=combine_gap)\n",
        "\n",
        "    if merged_segments:\n",
        "        return merged_segments\n",
        "    else:\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDpHO_5dUqcl"
      },
      "source": [
        "## Transcription using Sarvam AI API\n",
        "\n",
        "### Transcribe Audio Segment\n",
        "\n",
        "This function sends an audio segment to the Sarvam AI API for transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "40VjXm8AUrv6"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio_segment(start_time_sec, end_time_sec):\n",
        "    # Convert seconds to milliseconds for pydub\n",
        "    start_time_ms = start_time_sec * 1000\n",
        "    end_time_ms = end_time_sec * 1000\n",
        "\n",
        "    # Extract the audio segment\n",
        "    segment = audio[start_time_ms:end_time_ms]\n",
        "\n",
        "    # Export segment to an in-memory BytesIO object\n",
        "    audio_buffer = io.BytesIO()\n",
        "    segment.export(audio_buffer, format=\"wav\")\n",
        "    audio_buffer.seek(0)  # Reset buffer position to the beginning\n",
        "\n",
        "    files = {\n",
        "        'file': ('audiofile.wav', audio_buffer, 'audio/wav')\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, files=files, data=data)\n",
        "\n",
        "    if response.status_code == 200 or response.status_code == 201:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error for segment {start_time_sec}-{end_time_sec}: {response.status_code} - {response.text}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNsa79bCUs9c"
      },
      "source": [
        "### Write SRT File\n",
        "\n",
        "This function writes the transcription results into an SRT file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wWD810n0UuAk"
      },
      "outputs": [],
      "source": [
        "def write_srt_file(results, output_file_path):\n",
        "    \"\"\"\n",
        "    Writes the transcription results into an SRT file.\n",
        "\n",
        "    Args:\n",
        "        results (list): List of dictionaries containing 'start_time', 'end_time', and 'transcript'.\n",
        "        output_file_path (str): Path to save the SRT file.\n",
        "    \"\"\"\n",
        "    def format_timestamp(seconds):\n",
        "        \"\"\"Converts seconds to SRT timestamp format: hh:mm:ss,ms\"\"\"\n",
        "        milliseconds = int((seconds % 1) * 1000)\n",
        "        seconds = int(seconds)\n",
        "        minutes = seconds // 60\n",
        "        hours = minutes // 60\n",
        "        seconds = seconds % 60\n",
        "        minutes = minutes % 60\n",
        "        return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
        "\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
        "        for i, result in enumerate(results, start=1):\n",
        "            start_timestamp = format_timestamp(result[\"start_time\"])\n",
        "            end_timestamp = format_timestamp(result[\"end_time\"])\n",
        "            transcript = result[\"transcript\"]\n",
        "\n",
        "            # Write the SRT entry\n",
        "            srt_file.write(f\"{i}\\\\n\")\n",
        "            srt_file.write(f\"{start_timestamp} --> {end_timestamp}\\\\n\")\n",
        "            srt_file.write(f\"{transcript}\\\\n\\\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hei0Ni0Uvgv"
      },
      "source": [
        "## Main Execution\n",
        "\n",
        "### Set Up API and Audio File\n",
        "\n",
        "Set up the Sarvam AI API URL, headers, and data. Also, specify the path to the audio file.\n",
        "\n",
        "Note:- Please make sure to upload your audio (.wav) file, once uploaded please provide the correct audio_file_path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La0M6yBPUyGH"
      },
      "outputs": [],
      "source": [
        "api_url = \"https://api.sarvam.ai/speech-to-text-translate\"\n",
        "headers = {\n",
        "    \"api-subscription-key\" :SARVAM_AI_API\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"saaras:v2\",\n",
        "}\n",
        "audio_file_path = \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh6RmPfiUzlS"
      },
      "source": [
        "### Process Audio and Transcribe\n",
        "\n",
        "Process the audio file to detect speech segments and transcribe each segment using the Sarvam AI API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wmGtrVKU0ww",
        "outputId": "3c725536-214e-4a2e-9211-5ea03504322c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\HP/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
          ]
        }
      ],
      "source": [
        "\n",
        "timestamps = process_audio(audio_file_path)\n",
        "audio = AudioSegment.from_file(audio_file_path)\n",
        "\n",
        "results = []\n",
        "\n",
        "for start, end in timestamps:\n",
        "    transcription = transcribe_audio_segment(start, end)\n",
        "    if transcription is not None:\n",
        "        results.append({\n",
        "            \"start_time\": start,\n",
        "            \"end_time\": end,\n",
        "            \"transcript\": transcription[\"transcript\"]\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpJ-2cwlU2dc"
      },
      "source": [
        "### Save Results to SRT File\n",
        "\n",
        "Finally, save the transcription results to an SRT file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GZ4WQYeU3io",
        "outputId": "8e6fd1fa-b3d9-48fd-d39c-34e847ac3ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SRT file has been saved to subtitles.srt\n"
          ]
        }
      ],
      "source": [
        "output_srt_path = \"subtitles.srt\"\n",
        "write_srt_file(results, output_srt_path)\n",
        "\n",
        "print(f\"SRT file has been saved to {output_srt_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def srt_to_text(srt_path):\n",
        "    \"\"\"\n",
        "    Convert SRT file content to plain text.\n",
        "    \"\"\"\n",
        "    text = []\n",
        "\n",
        "    with open(srt_path, 'r', encoding='utf-8') as file:\n",
        "        for content in file:\n",
        "            lines = content.split('\\\\n')\n",
        "            for line in lines:\n",
        "                if line.strip().isdigit() or '-->' in line or line.strip() == '':\n",
        "                    continue\n",
        "                text.append(line)\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "content = srt_to_text(output_srt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"More than half of the patients in states like Punjab, Gujarat, UP, and Madhya Pradesh have recovered. Maharashtra Bihar's drama, recovery rate slow due to new patients.\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdP1ILf1U40Y"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates how to process an audio file, detect speech segments using VAD, transcribe those segments using the Sarvam AI API, and save the results in an SRT file format. You can modify the configuration parameters and API settings to suit your specific needs.\n",
        "\n",
        "\n",
        "\n",
        "### **Additional Resources**\n",
        "\n",
        "For more details, refer to the official **Saaras API documentation** and join the community for support:\n",
        "\n",
        "- **Documentation**: [docs.sarvam.ai](https://docs.sarvam.ai/)\n",
        "- **Community**: [Join the Discord Community](https://discord.gg/hTuVuPNF)\n",
        "\n",
        "### **Notes:**\n",
        "\n",
        "**File Format:** Ensure the file is in .wav format and has a sample rate of 16kHz.\n",
        "\n",
        "**API Key:** Double-check that the SARVAM_API_KEY is correctly set.\n",
        "\n",
        "**Error Handling:** If transcription fails, the error message and response content will be displayed for debugging.\n",
        "\n",
        "**Keep Building!** ðŸš€\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
