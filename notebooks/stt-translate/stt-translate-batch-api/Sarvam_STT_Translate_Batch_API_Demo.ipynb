{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njEMu7DlvMUn"
      },
      "source": [
        "# **Instructions to Keep in Mind Before Running the Notebook:**\n",
        "\n",
        "1. **Get the Subscription Key**:  \n",
        "   Go to [dashboard.sarvam.ai](https://dashboard.sarvam.ai) and copy your **API-Subscription Key**.\n",
        "\n",
        "2. **Upload Files**:  \n",
        "   Use the **Upload** button in the Jupyter notebook to upload the files (e.g., audio files) you want to process.\n",
        "\n",
        "3. **Set File Path**:  \n",
        "   Modify the file path in the code to match the path of your uploaded file, e.g., file_path = '/path/to/your/file.wav'.\n",
        "\n",
        "4. **Set Download Directory**:  \n",
        "   Change the directory path where you want the files to be saved after download, e.g., destination_dir = '/path/to/your/local/directory'.\n",
        "\n",
        "5. **Run the Code**:  \n",
        "   Execute the code with the correct **API key**, **file paths**, and **download directory**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R6WPZGv5_2_"
      },
      "source": [
        "\n",
        "## 1. Setup and Configuration\n",
        "\n",
        "First, let's install the required packages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJvMhkZ6vPTb"
      },
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "First, let's install the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr1wkbGUvHyH",
        "outputId": "4ba0458e-c013-42af-fce3-c343a268eaaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-storage-file-datalake\n",
            "  Downloading azure_storage_file_datalake-12.18.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-file-datalake)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting azure-storage-blob>=12.24.1 (from azure-storage-file-datalake)\n",
            "  Downloading azure_storage_blob-12.24.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-file-datalake) (4.12.2)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-file-datalake)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (1.17.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob>=12.24.1->azure-storage-file-datalake) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (2.22)\n",
            "Downloading azure_storage_file_datalake-12.18.1-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_blob-12.24.1-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m408.4/408.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, aiofiles, azure-core, azure-storage-blob, azure-storage-file-datalake\n",
            "Successfully installed aiofiles-24.1.0 azure-core-1.32.0 azure-storage-blob-12.24.1 azure-storage-file-datalake-12.18.1 isodate-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install azure-storage-file-datalake aiofiles aiohttp requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6AO6tIgvVIc"
      },
      "source": [
        "Now import the necessary libraries and set up logging:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFILCZmMvcjm"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import aiofiles\n",
        "import requests\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "from azure.storage.filedatalake.aio import DataLakeDirectoryClient, FileSystemClient\n",
        "from azure.storage.filedatalake import ContentSettings\n",
        "import mimetypes\n",
        "import logging\n",
        "from pprint import pprint\n",
        "import os\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration\n",
        "API_SUBSCRIPTION_KEY = '<YOUR_API_KEY>'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkjNgOTmvgZP"
      },
      "source": [
        "## 2. SarvamClient Class\n",
        "\n",
        "The SarvamClient class handles all Batch operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGItkUpvtr7"
      },
      "outputs": [],
      "source": [
        "class SarvamClient:\n",
        "    def __init__(self, url: str):\n",
        "        self.account_url, self.file_system_name, self.directory_name, self.sas_token = (\n",
        "            self._extract_url_components(url)\n",
        "        )\n",
        "        self.lock = asyncio.Lock()\n",
        "        print(f\"Initialized SarvamClient with directory: {self.directory_name}\")\n",
        "\n",
        "    def update_url(self, url: str):\n",
        "        self.account_url, self.file_system_name, self.directory_name, self.sas_token = (\n",
        "            self._extract_url_components(url)\n",
        "        )\n",
        "        print(f\"Updated URL to directory: {self.directory_name}\")\n",
        "\n",
        "    def _extract_url_components(self, url: str):\n",
        "        parsed_url = urlparse(url)\n",
        "        account_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\".replace(\n",
        "            \".blob.\", \".dfs.\"\n",
        "        )\n",
        "        path_components = parsed_url.path.strip(\"/\").split(\"/\")\n",
        "        file_system_name = path_components[0]\n",
        "        directory_name = \"/\".join(path_components[1:])\n",
        "        sas_token = parsed_url.query\n",
        "        return account_url, file_system_name, directory_name, sas_token\n",
        "\n",
        "    async def upload_files(self, local_file_paths, overwrite=True):\n",
        "        print(f\"Starting upload of {len(local_file_paths)} files\")\n",
        "        async with DataLakeDirectoryClient(\n",
        "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
        "            file_system_name=self.file_system_name,\n",
        "            directory_name=self.directory_name,\n",
        "            credential=None,\n",
        "        ) as directory_client:\n",
        "            tasks = []\n",
        "            for path in local_file_paths:\n",
        "                file_name = path.split(\"/\")[-1]\n",
        "                tasks.append(\n",
        "                    self._upload_file(directory_client, path, file_name, overwrite)\n",
        "                )\n",
        "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "            print(f\"Upload completed for {sum(1 for r in results if not isinstance(r, Exception))} files\")\n",
        "\n",
        "    async def _upload_file(self, directory_client, local_file_path, file_name, overwrite=True):\n",
        "        try:\n",
        "            async with aiofiles.open(local_file_path, mode=\"rb\") as file_data:\n",
        "                mime_type = mimetypes.guess_type(local_file_path)[0] or \"audio/wav\"\n",
        "                file_client = directory_client.get_file_client(file_name)\n",
        "                data = await file_data.read()\n",
        "                await file_client.upload_data(\n",
        "                    data,\n",
        "                    overwrite=overwrite,\n",
        "                    content_settings=ContentSettings(content_type=mime_type),\n",
        "                )\n",
        "                print(f\"âœ… File uploaded successfully: {file_name}\")\n",
        "                print(f\"   Type: {mime_type}\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Upload failed for {file_name}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def list_files(self):\n",
        "        print(\"\\nğŸ“‚ Listing files in directory...\")\n",
        "        file_names = []\n",
        "        async with FileSystemClient(\n",
        "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
        "            file_system_name=self.file_system_name,\n",
        "            credential=None,\n",
        "        ) as file_system_client:\n",
        "            async for path in file_system_client.get_paths(self.directory_name):\n",
        "                file_name = path.name.split(\"/\")[-1]\n",
        "                async with self.lock:\n",
        "                    file_names.append(file_name)\n",
        "        print(f\"Found {len(file_names)} files:\")\n",
        "        for file in file_names:\n",
        "            print(f\"   ğŸ“„ {file}\")\n",
        "        return file_names\n",
        "\n",
        "    async def download_files(self, file_names, destination_dir):\n",
        "        print(f\"\\nâ¬‡ï¸ Starting download of {len(file_names)} files to {destination_dir}\")\n",
        "        async with DataLakeDirectoryClient(\n",
        "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
        "            file_system_name=self.file_system_name,\n",
        "            directory_name=self.directory_name,\n",
        "            credential=None,\n",
        "        ) as directory_client:\n",
        "            tasks = []\n",
        "            for file_name in file_names:\n",
        "                tasks.append(\n",
        "                    self._download_file(directory_client, file_name, destination_dir)\n",
        "                )\n",
        "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "            print(f\"Download completed for {sum(1 for r in results if not isinstance(r, Exception))} files\")\n",
        "\n",
        "    async def _download_file(self, directory_client, file_name, destination_dir):\n",
        "        try:\n",
        "            file_client = directory_client.get_file_client(file_name)\n",
        "            download_path = f\"{destination_dir}/{file_name}\"\n",
        "            async with aiofiles.open(download_path, mode=\"wb\") as file_data:\n",
        "                stream = await file_client.download_file()\n",
        "                data = await stream.readall()\n",
        "                await file_data.write(data)\n",
        "            print(f\"âœ… Downloaded: {file_name} -> {download_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Download failed for {file_name}: {str(e)}\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id2zfGrHv--l"
      },
      "source": [
        "## 3. Sarvam AI API Integration\n",
        "\n",
        "These functions handle the Speech-to-Text job lifecycle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FxvpEMTwA5P"
      },
      "outputs": [],
      "source": [
        "async def initialize_job():\n",
        "    print(\"\\\\nğŸš€ Initializing job...\")\n",
        "    url = 'https://api.sarvam.ai/speech-to-text-translate/job/init'\n",
        "    headers = {'API-Subscription-Key': API_SUBSCRIPTION_KEY}\n",
        "    response = requests.post(url, headers=headers)\n",
        "    print(\"\\\\nInitialize Job Response:\")\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "    print(\"Response Body:\")\n",
        "    pprint(response.json() if response.status_code == 202 else response.text)\n",
        "\n",
        "    if response.status_code == 202:\n",
        "        return response.json()\n",
        "    return None\n",
        "\n",
        "async def check_job_status(job_id):\n",
        "    print(f\"\\\\nğŸ” Checking status for job: {job_id}\")\n",
        "    url = f'https://api.sarvam.ai/speech-to-text-translate/job/{job_id}/status'\n",
        "    headers = {'API-Subscription-Key': API_SUBSCRIPTION_KEY}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(\"\\\\nJob Status Response:\")\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "    print(\"Response Body:\")\n",
        "    pprint(response.json() if response.status_code == 200 else response.text)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    return None\n",
        "\n",
        "async def start_job(job_id):\n",
        "    print(f\"\\\\nâ–¶ï¸ Starting job: {job_id}\")\n",
        "    url = 'https://api.sarvam.ai/speech-to-text-translate/job'\n",
        "    headers = {\n",
        "        'API-Subscription-Key': API_SUBSCRIPTION_KEY,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    data = {\n",
        "        \"job_id\": job_id,\n",
        "        \"job_parameters\": {\n",
        "\n",
        "        }\n",
        "    }\n",
        "    print(\"\\\\nRequest Body:\")\n",
        "    pprint(data)\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    print(\"\\\\nStart Job Response:\")\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "    print(\"Response Body:\")\n",
        "    pprint(response.json() if response.status_code == 200 else response.text)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usBfks2jyeJ5",
        "outputId": "9db6cc50-ce10-464e-c18b-ed0a46026f3e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAC7-sUgwEYw"
      },
      "source": [
        "## 4. Main Execution Flow\n",
        "\n",
        "Here's the main function that orchestrates the entire process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqagsZaMwE7B",
        "outputId": "f0acd321-8905-4b44-e9fd-a64f12a428c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\n=== Starting Speech-to-Text Processing ===\n",
            "\\nğŸš€ Initializing job...\n",
            "\\nInitialize Job Response:\n",
            "Status Code: 202\n",
            "Response Body:\n",
            "{'input_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/inputs?se=2025-02-20T10%3A59%3A07Z&sp=wdl&sv=2025-01-05&sr=d&sdd=5&sig=%2Bli06%2BK5JkpTqmHWs3GyP3XiVf5l2en5hHbmbcHTwXw%3D',\n",
            " 'job_id': '20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca',\n",
            " 'output_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/outputs?se=2025-02-27T09%3A59%3A07Z&sp=rl&sv=2025-01-05&sr=d&sdd=5&sig=XOX5d9Bz4v9vNLvFCHWhehxkLppMJmcWd4lR1m5Sow8%3D',\n",
            " 'storage_container_type': 'Azure'}\n",
            "\\nğŸ“¤ Uploading files to input storage: https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/inputs?se=2025-02-20T10%3A59%3A07Z&sp=wdl&sv=2025-01-05&sr=d&sdd=5&sig=%2Bli06%2BK5JkpTqmHWs3GyP3XiVf5l2en5hHbmbcHTwXw%3D\n",
            "Initialized SarvamClient with directory: jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/inputs\n",
            "Files to upload: ['arvind-gujarati.wav']\n",
            "Starting upload of 1 files\n",
            "âœ… File uploaded successfully: arvind-gujarati.wav\n",
            "   Type: audio/x-wav\n",
            "Upload completed for 1 files\n",
            "\\nâ–¶ï¸ Starting job: 20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca\n",
            "\\nRequest Body:\n",
            "{'job_id': '20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca',\n",
            " 'job_parameters': {}}\n",
            "\\nStart Job Response:\n",
            "Status Code: 200\n",
            "Response Body:\n",
            "{'job_status': {'created_at': '2025-02-20T09:59:07.429084+00:00',\n",
            "                'error_message': '',\n",
            "                'failed_files_count': 0,\n",
            "                'input_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/inputs?se=2025-02-20T10%3A59%3A07Z&sp=wdl&sv=2025-01-05&sr=d&sdd=5&sig=%2Bli06%2BK5JkpTqmHWs3GyP3XiVf5l2en5hHbmbcHTwXw%3D',\n",
            "                'job_details': [],\n",
            "                'job_id': '20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca',\n",
            "                'job_state': 'Pending',\n",
            "                'output_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/outputs?se=2025-02-27T09%3A59%3A07Z&sp=rl&sv=2025-01-05&sr=d&sdd=5&sig=XOX5d9Bz4v9vNLvFCHWhehxkLppMJmcWd4lR1m5Sow8%3D',\n",
            "                'owner_id': 'ba3a72b2144ea99c97e390e5f0f43780c693171b6fd7f68ad7dde2f62dcb801a',\n",
            "                'storage_container_type': 'Azure',\n",
            "                'successful_files_count': 0,\n",
            "                'total_files': 0,\n",
            "                'updated_at': '2025-02-20T09:59:10.965676+00:00'}}\n",
            "\\nâ³ Monitoring job status...\n",
            "\\nStatus check attempt 1\n",
            "\\nğŸ” Checking status for job: 20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca\n",
            "\\nJob Status Response:\n",
            "Status Code: 200\n",
            "Response Body:\n",
            "{'created_at': '2025-02-20T09:59:07.429084+00:00',\n",
            " 'error_message': '',\n",
            " 'failed_files_count': 0,\n",
            " 'input_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/inputs?se=2025-02-20T10%3A59%3A07Z&sp=wdl&sv=2025-01-05&sr=d&sdd=5&sig=%2Bli06%2BK5JkpTqmHWs3GyP3XiVf5l2en5hHbmbcHTwXw%3D',\n",
            " 'job_details': [],\n",
            " 'job_id': '20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca',\n",
            " 'job_state': 'Running',\n",
            " 'output_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/outputs?se=2025-02-27T09%3A59%3A07Z&sp=rl&sv=2025-01-05&sr=d&sdd=5&sig=XOX5d9Bz4v9vNLvFCHWhehxkLppMJmcWd4lR1m5Sow8%3D',\n",
            " 'owner_id': 'ba3a72b2144ea99c97e390e5f0f43780c693171b6fd7f68ad7dde2f62dcb801a',\n",
            " 'storage_container_type': 'Azure',\n",
            " 'successful_files_count': 0,\n",
            " 'total_files': 0,\n",
            " 'updated_at': '2025-02-20T09:59:11.331306+00:00'}\n",
            "â³ Current status: Running\n",
            "\\nStatus check attempt 2\n",
            "\\nğŸ” Checking status for job: 20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca\n",
            "\\nJob Status Response:\n",
            "Status Code: 200\n",
            "Response Body:\n",
            "{'created_at': '2025-02-20T09:59:07.429084+00:00',\n",
            " 'error_message': '',\n",
            " 'failed_files_count': 0,\n",
            " 'input_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/inputs?se=2025-02-20T10%3A59%3A07Z&sp=wdl&sv=2025-01-05&sr=d&sdd=5&sig=%2Bli06%2BK5JkpTqmHWs3GyP3XiVf5l2en5hHbmbcHTwXw%3D',\n",
            " 'job_details': [{'error_message': '',\n",
            "                  'file_id': '0',\n",
            "                  'file_name': 'arvind-gujarati.wav',\n",
            "                  'state': 'Success'}],\n",
            " 'job_id': '20250220_b26fa685-5af2-4f50-89c3-13f4dbb242ca',\n",
            " 'job_state': 'Completed',\n",
            " 'output_storage_path': 'https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/outputs?se=2025-02-27T09%3A59%3A07Z&sp=rl&sv=2025-01-05&sr=d&sdd=5&sig=XOX5d9Bz4v9vNLvFCHWhehxkLppMJmcWd4lR1m5Sow8%3D',\n",
            " 'owner_id': 'ba3a72b2144ea99c97e390e5f0f43780c693171b6fd7f68ad7dde2f62dcb801a',\n",
            " 'storage_container_type': 'Azure',\n",
            " 'successful_files_count': 1,\n",
            " 'total_files': 1,\n",
            " 'updated_at': '2025-02-20T09:59:12.096475+00:00'}\n",
            "âœ… Job completed successfully!\n",
            "\n",
            "ğŸ“¥ Downloading results from: https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/outputs?se=2025-02-27T09%3A59%3A07Z&sp=rl&sv=2025-01-05&sr=d&sdd=5&sig=XOX5d9Bz4v9vNLvFCHWhehxkLppMJmcWd4lR1m5Sow8%3D\n",
            "Updated URL to directory: jobs/2025-02-20/SPEECH_TO_TEXT_TRANSLATE_BULK/b26fa685-5af2-4f50-89c3-13f4dbb242ca/outputs\n",
            "\n",
            "ğŸ“‚ Listing files in directory...\n",
            "Found 1 files:\n",
            "   ğŸ“„ 0.json\n",
            "Files to download: ['0.json']\n",
            "\n",
            "â¬‡ï¸ Starting download of 1 files to /content/data\n",
            "âœ… Downloaded: 0.json -> /content/data/0.json\n",
            "Download completed for 1 files\n",
            "Files have been downloaded to: /content/data\n",
            "\\n=== Processing Complete ===\n"
          ]
        }
      ],
      "source": [
        "async def main():\n",
        "    print(\"\\\\n=== Starting Speech-to-Text Processing ===\")\n",
        "\n",
        "    # Step 1: Initialize the job\n",
        "    job_info = await initialize_job()\n",
        "    if not job_info:\n",
        "        print(\"âŒ Job initialization failed\")\n",
        "        return\n",
        "\n",
        "    job_id = job_info['job_id']\n",
        "    input_storage_path = job_info['input_storage_path']\n",
        "    output_storage_path = job_info['output_storage_path']\n",
        "\n",
        "    # Step 2: Upload files\n",
        "    print(f\"\\\\nğŸ“¤ Uploading files to input storage: {input_storage_path}\")\n",
        "    client = SarvamClient(input_storage_path)\n",
        "    local_files = [\"arvind-gujarati.wav\"]  # Replace with your audio files\n",
        "    print(f\"Files to upload: {local_files}\")\n",
        "    await client.upload_files(local_files)\n",
        "\n",
        "    # Step 3: Start the job\n",
        "    job_start_response = await start_job(job_id)\n",
        "    if not job_start_response:\n",
        "        print(\"âŒ Failed to start job\")\n",
        "        return\n",
        "\n",
        "    # Step 4: Monitor job status\n",
        "    print(\"\\\\nâ³ Monitoring job status...\")\n",
        "    attempt = 1\n",
        "    while True:\n",
        "        print(f\"\\\\nStatus check attempt {attempt}\")\n",
        "        job_status = await check_job_status(job_id)\n",
        "        if not job_status:\n",
        "            print(\"âŒ Failed to get job status\")\n",
        "            break\n",
        "\n",
        "        status = job_status['job_state']\n",
        "        if status == 'Completed':\n",
        "            print(\"âœ… Job completed successfully!\")\n",
        "            break\n",
        "        elif status == 'Failed':\n",
        "            print(\"âŒ Job failed!\")\n",
        "            break\n",
        "        else:\n",
        "            print(f\"â³ Current status: {status}\")\n",
        "            await asyncio.sleep(10)\n",
        "        attempt += 1\n",
        "\n",
        "    # Step 5: Download results\n",
        "    # Step 5: Download results\n",
        "    if status == 'Completed':\n",
        "        print(f\"\\nğŸ“¥ Downloading results from: {output_storage_path}\")\n",
        "        client.update_url(output_storage_path)  # Update URL to the file path\n",
        "\n",
        "        # List all the files you want to download\n",
        "        files = await client.list_files()\n",
        "\n",
        "        if not files:\n",
        "            print(\"âŒ No files found to download.\")\n",
        "        else:\n",
        "            print(f\"Files to download: {files}\")\n",
        "\n",
        "            # Specify the local destination directory\n",
        "            destination_dir = \"/content/data\"  # Set this to the local path you want\n",
        "\n",
        "            # Make sure the directory exists before downloading\n",
        "            os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Download the files to the local directory\n",
        "                await client.download_files(files, destination_dir=destination_dir)\n",
        "                print(f\"Files have been downloaded to: {destination_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error during file download: {e}\")\n",
        "\n",
        "        print(\"\\\\n=== Processing Complete ===\")\n",
        "\n",
        "\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    await main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzmN1XP2-Ydl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}