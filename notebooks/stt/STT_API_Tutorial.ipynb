{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **STT API Tutorial**\n",
        "\n",
        "This notebook provides a step-by-step guide on how to use the STT API for speech-to-text tasks. It includes instructions for installation, setting up the API key, uploading audio files, and using the API for transcription and translation.*italicised text*\n",
        "\n"
      ],
      "metadata": {
        "id": "h6LiHbLsHGbO"
      },
      "id": "h6LiHbLsHGbO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Installation**\n",
        "Before you begin, ensure you have the necessary Python libraries installed. Run the following commands to install the required packages:\n"
      ],
      "metadata": {
        "id": "Vvf7s2t8H3EO"
      },
      "id": "Vvf7s2t8H3EO"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas pydub\n"
      ],
      "metadata": {
        "id": "OvZ0-mhCoN8-",
        "outputId": "1a5cdb38-64f4-418e-9217-a2e5fcc85c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OvZ0-mhCoN8-",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Import Required Libraries**\n",
        "\n",
        "This section imports the necessary Python libraries for making HTTP requests, handling audio files, and managing data."
      ],
      "metadata": {
        "id": "mVA_CIlg64od"
      },
      "id": "mVA_CIlg64od"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8bc4d76f-ca04-4641-871c-91e53748f387",
      "metadata": {
        "id": "8bc4d76f-ca04-4641-871c-91e53748f387"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Set Up the API Endpoint and Payload**\n",
        "\n",
        "To use the Saaras API, you need an API subscription key. Follow these steps to set up your API key:\n",
        "\n",
        "1. **Obtain your API key**: If you don‚Äôt have an API key, sign up on the [Sarvam AI Dashboard](https://dashboard.sarvam.ai/) to get one.\n",
        "2. **Replace the placeholder key**: In the code below, replace \"YOUR_SARVAM_AI_API_KEY\" with your actual API key."
      ],
      "metadata": {
        "id": "Eqdfvakn8Kw_"
      },
      "id": "Eqdfvakn8Kw_"
    },
    {
      "cell_type": "code",
      "source": [
        "SARVAM_AI_API=\"YOUR_SARVAM_AI_API_KEY\""
      ],
      "metadata": {
        "id": "Kp_kAaY0dtA9"
      },
      "id": "Kp_kAaY0dtA9",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***2.1 Setting Up the API Endpoint and Payload***\n",
        "\n",
        "This section defines the API endpoint and the payload for the translation request. Replace the placeholder values with your actual API key and desired parameters."
      ],
      "metadata": {
        "id": "e8cMSRCudveP"
      },
      "id": "e8cMSRCudveP"
    },
    {
      "cell_type": "code",
      "source": [
        "# API endpoint for speech-to-text\n",
        "api_url = \"https://api.sarvam.ai/speech-to-text\"\n",
        "\n",
        "# Headers containing the API subscription key\n",
        "headers = {\n",
        "    \"api-subscription-key\": SARVAM_AI_API  # Replace with your API key\n",
        "}\n",
        "\n",
        "# Data payload for the transcription request\n",
        "data = {\n",
        "    \"language_code\": \"hi-IN\",  # Specify the language of the audio (e.g., 'hi-IN' for Hindi)\n",
        "    \"model\": \"saarika:v2\",     # Specify the model to be used for transcription\n",
        "    \"with_timestamps\": False   # Set to True if you want word-level timestamps\n",
        "}"
      ],
      "metadata": {
        "id": "SPSBBgRc6Zjm"
      },
      "id": "SPSBBgRc6Zjm",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Uploading Audio Files**\n",
        "\n",
        "To translate audio, you need to upload a `.wav` file. Follow these steps:\n",
        "\n",
        "1. **Prepare your audio file**: Ensure your audio file is in `.wav` format. If your file is in a different format, you can use tools like `pydub` to convert it.\n",
        "2. **Upload the file**: If you're using Google Colab, you can upload the file using the file uploader:"
      ],
      "metadata": {
        "id": "qaC3-oLTeZ8T"
      },
      "id": "qaC3-oLTeZ8T"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "audio_file_path = list(uploaded.keys())[0]  # Get the name of the uploaded file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "9ekiIlxBeeV5",
        "outputId": "9691235e-ec4d-4c27-ed1e-cae6714c3138"
      },
      "id": "9ekiIlxBeeV5",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98aa77e4-0c20-4c7a-967d-9c0e99dc38d9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98aa77e4-0c20-4c7a-967d-9c0e99dc38d9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.wav to test (2).wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Define the `split_audio` Function**\n",
        "\n",
        "This function splits an audio file into smaller chunks of a specified duration. This is useful for processing long audio files that exceed the API's input length limit."
      ],
      "metadata": {
        "id": "xphzaATf6-6e"
      },
      "id": "xphzaATf6-6e"
    },
    {
      "cell_type": "code",
      "source": [
        "def split_audio(audio_path, chunk_duration_ms):\n",
        "    \"\"\"\n",
        "    Splits an audio file into smaller chunks of specified duration.\n",
        "\n",
        "    Args:\n",
        "        audio_path (str): Path to the audio file to be split.\n",
        "        chunk_duration_ms (int): Duration of each chunk in milliseconds.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of AudioSegment objects representing the audio chunks.\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_file(audio_path)  # Load the audio file\n",
        "    chunks = []\n",
        "    if len(audio) > chunk_duration_ms:\n",
        "        # Split the audio into chunks of the specified duration\n",
        "        for i in range(0, len(audio), chunk_duration_ms):\n",
        "            chunks.append(audio[i:i + chunk_duration_ms])\n",
        "    else:\n",
        "        # If the audio is shorter than the chunk duration, use the entire audio\n",
        "        chunks.append(audio)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "QeFprHO1uSrV"
      },
      "id": "QeFprHO1uSrV",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Define the `transcribe_audio_chunks` Function**\n",
        "\n",
        "This function transcribes audio chunks using the Saaras API. It handles the API request for each chunk and collates the results.\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "bm0pktY98Ccs"
      },
      "id": "bm0pktY98Ccs"
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_chunks(audio_file_path, api_url, headers, data, chunk_duration_ms=5*60*1000):\n",
        "    \"\"\"\n",
        "    Transcribes audio chunks using the Speech-to-Text API.\n",
        "\n",
        "    Args:\n",
        "        audio_file_path (str): Path to the audio file.\n",
        "        api_url (str): The API endpoint URL for Speech-to-Text.\n",
        "        headers (dict): Headers containing authentication information.\n",
        "        data (dict): Data payload for the transcription API.\n",
        "        chunk_duration_ms (int): Duration of each audio chunk in milliseconds.\n",
        "\n",
        "    Returns:\n",
        "        dict: Collated response containing the transcript.\n",
        "    \"\"\"\n",
        "    # Split the audio into chunks\n",
        "    chunks = split_audio(audio_file_path, chunk_duration_ms)\n",
        "    responses = []  # List to store the transcription results\n",
        "\n",
        "    # Process each chunk\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        # Export the chunk to a BytesIO object (in-memory binary stream)\n",
        "        chunk_buffer = io.BytesIO()\n",
        "        chunk.export(chunk_buffer, format=\"wav\")\n",
        "        chunk_buffer.seek(0)  # Reset the pointer to the start of the stream\n",
        "\n",
        "        # Prepare the file for the API request\n",
        "        files = {'file': ('audiofile.wav', chunk_buffer, 'audio/wav')}\n",
        "\n",
        "        try:\n",
        "            # Make the POST request to the API\n",
        "            response = requests.post(api_url, headers=headers, files=files, data=data)\n",
        "            if response.status_code == 200 or response.status_code == 201:\n",
        "                print(f\"Chunk {idx} POST Request Successful!\")\n",
        "                response_data = response.json()\n",
        "                transcript = response_data.get(\"transcript\", \"\")\n",
        "                responses.append({\"transcript\": transcript})\n",
        "            else:\n",
        "                # Handle failed requests\n",
        "                print(f\"Chunk {idx} POST Request failed with status code: {response.status_code}\")\n",
        "                print(\"Response:\", response.text)\n",
        "        except Exception as e:\n",
        "            # Handle any exceptions during the request\n",
        "            print(f\"Error processing chunk {idx}: {e}\")\n",
        "        finally:\n",
        "            # Ensure the buffer is closed after processing\n",
        "            chunk_buffer.close()\n",
        "\n",
        "    # Collate the transcriptions from all chunks\n",
        "    collated_responses = {\"collated_transcript\": \" \".join([i[\"transcript\"] for i in responses])}\n",
        "    return collated_responses"
      ],
      "metadata": {
        "id": "WCpE_Rv16Rjl"
      },
      "id": "WCpE_Rv16Rjl",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Transcribe the Audio**\n",
        "\n",
        "This section calls the `transcribe_audio_chunks` function to transcribe the audio file. Replace `audio_file_path` with the path to your audio file"
      ],
      "metadata": {
        "id": "7CorqAF18cZF"
      },
      "id": "7CorqAF18cZF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the audio file to be transcribed\n",
        "# audio_file_path = \"test.wav\"  # Replace with your file path\n",
        "\n",
        "# Transcribe the audio\n",
        "transcriptions = transcribe_audio_chunks(audio_file_path, api_url, headers, data)\n",
        "\n",
        "# Display the transcription results\n",
        "transcriptions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9N7-WBw6jS8",
        "outputId": "5ffbcb7b-9e46-4cfe-f0d0-ffca240fd376"
      },
      "id": "m9N7-WBw6jS8",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 0 POST Request Successful!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'collated_transcript': '‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡§æ‡§≠ ‡§∏‡•á ‡§™‡•ç‡§∞‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Ö‡§®‡•á‡§ï ‡§®‡•à‡§§‡§ø‡§ï ‡§ï‡§π‡§æ‡§®‡§ø‡§Ø‡§æ‡§Å ‡§π‡•à‡§Ç‡•§ ‡§µ‡•á ‡§Ü‡§™‡§ï‡•á ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ï‡•Ä ‡§ï‡§≤‡•ç‡§™‡§®‡§æ ‡§ï‡•ã ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç, ‡§â‡§®‡§ï‡§æ ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§ñ‡•Å‡§∂ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§´‡•Ç‡§ü‡•Ä ‡§®‡•à‡§§‡§ø‡§ï ‡§ï‡§π‡§æ‡§®‡§ø‡§Ø‡§æ‡§Å ‡§â‡§®‡§ï‡§æ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡§®‡•á ‡§î‡§∞ ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§∞‡§ñ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§¶‡§∞‡•ç‡§∂ ‡§π‡•à‡§Ç‡•§'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Explanation of the Output**\n",
        "\n",
        "The output of the `transcribe_audio_chunks` function is a dictionary containing the collated transcript of the entire audio file. If the audio was split into multiple chunks, the transcripts from all chunks are combined into a single string.\n",
        "\n",
        "Example output:\n",
        "\n",
        "```{\n",
        "    \"collated_transcript\": \"This is the transcribed text from the audio file.\"\n",
        "}```\n",
        "\n",
        "### **7. Conclusion**\n",
        "\n",
        "This tutorial demonstrated how to use the **STT API** for speech-to-text transcription. By following the steps, you can transcribe audio files, even long ones, by splitting them into smaller chunks. The process involves installing required libraries, setting up your API key, uploading audio, and transcribing it using the provided functions.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Additional Resources**\n",
        "\n",
        "For more details, refer to the our official documentation and we are always there to support and help you on our Discord Server:\n",
        "\n",
        "- **Documentation**: [docs.sarvam.ai](https://docs.sarvam.ai)  \n",
        "- **Community**: [Join the Discord Community](https://discord.gg/hTuVuPNF)\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Final Notes**\n",
        "\n",
        "- Keep your API key secure.\n",
        "- Use clear audio for best results.\n",
        "- Explore advanced features like diarization and translation.\n",
        "\n",
        "**Keep Building!** üöÄ"
      ],
      "metadata": {
        "id": "seDzWdnC83Yk"
      },
      "id": "seDzWdnC83Yk"
    },
    {
      "cell_type": "markdown",
      "id": "52499a91-387b-4d89-b34f-d32549a728ec",
      "metadata": {
        "id": "52499a91-387b-4d89-b34f-d32549a728ec"
      },
      "source": [
        "## Using Saaras for subtitles\n",
        "\n",
        "you can easily overcome the one-inch tall barrier of subtitles using Saaras. Generate custom subtitles for your audios using a combination of Voice Activity Detection and Saaras's output and save the subtitles in an .srt file for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6caa9c5c-dfb7-4564-b16b-f0f8fafaebe9",
      "metadata": {
        "id": "6caa9c5c-dfb7-4564-b16b-f0f8fafaebe9",
        "outputId": "7a037971-7622-4844-ab2a-f9267a9a745a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRT file has been saved to subtitles.srt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "\n",
        "sample_rate = 16000  # Set the sample rate for loading audio\n",
        "vad_threshold = 0.5  # Threshold for VAD\n",
        "combine_duration = 8  # Maximum duration for combined segments\n",
        "combine_gap = 1  # Maximum gap between segments to combine\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_vad_probs(model, audio, sample_rate=16000):\n",
        "    audio = torch.as_tensor(audio, dtype=torch.float32)\n",
        "    window_size_samples = 512 if sample_rate == 16000 else 256\n",
        "\n",
        "    model.reset_states()\n",
        "    audio_length_samples = len(audio)\n",
        "\n",
        "    speech_probs = []\n",
        "    for current_start_sample in range(0, audio_length_samples, window_size_samples):\n",
        "        chunk = audio[current_start_sample: current_start_sample + window_size_samples]\n",
        "        if len(chunk) < window_size_samples:\n",
        "            chunk = torch.nn.functional.pad(chunk, (0, int(window_size_samples - len(chunk))))\n",
        "        speech_prob = model(chunk, sample_rate).item()\n",
        "        speech_probs.append(speech_prob)\n",
        "\n",
        "    return speech_probs\n",
        "\n",
        "def get_utterances(vad_probs, threshold=0.5, frame_duration=0.032):\n",
        "    \"\"\"Extracts utterances (start and end times) based on VAD probabilities.\"\"\"\n",
        "    utterances = []\n",
        "    in_utterance = False\n",
        "    utterance_start = 0\n",
        "\n",
        "    for i, prob in enumerate(vad_probs):\n",
        "        if prob > threshold and not in_utterance:\n",
        "            in_utterance = True\n",
        "            utterance_start = i * frame_duration\n",
        "        elif prob <= threshold and in_utterance:\n",
        "            in_utterance = False\n",
        "            utterance_end = i * frame_duration\n",
        "            if utterance_end - utterance_start > 0:\n",
        "                utterances.append((utterance_start, utterance_end))\n",
        "\n",
        "    if in_utterance:\n",
        "        utterances.append((utterance_start, len(vad_probs) * frame_duration))\n",
        "\n",
        "    return utterances\n",
        "\n",
        "def merge_segments(segments, max_duration=8, max_gap=1):\n",
        "    \"\"\"Combines segments with pauses shorter than `max_gap` seconds, with total duration limit.\"\"\"\n",
        "    merged_segments = []\n",
        "    if not segments:\n",
        "        return merged_segments  # Return empty if no segments are found\n",
        "\n",
        "    current_start, current_end = segments[0]\n",
        "\n",
        "    for start, end in segments[1:]:\n",
        "        combined_duration = (end - current_start)\n",
        "\n",
        "        if (start - current_end <= max_gap) and (combined_duration <= max_duration):\n",
        "            current_end = end\n",
        "        else:\n",
        "            merged_segments.append((current_start, current_end))\n",
        "            current_start, current_end = start, end\n",
        "\n",
        "    merged_segments.append((current_start, current_end))\n",
        "    return merged_segments\n",
        "\n",
        "\n",
        "def process_audio(audio_file):\n",
        "    vad_model, _ = torch.hub.load(\n",
        "        repo_or_dir='snakers4/silero-vad',\n",
        "        model='silero_vad',\n",
        "        force_reload=False,\n",
        "        onnx=False\n",
        "    )\n",
        "    vad_model.eval()\n",
        "\n",
        "    audio, _ = librosa.load(audio_file, sr=sample_rate)\n",
        "    speech_probs = get_vad_probs(vad_model, audio, sample_rate)\n",
        "    utterances = get_utterances(speech_probs, threshold=vad_threshold)\n",
        "\n",
        "    if not utterances:\n",
        "        print(f\"No VAD regions detected for {audio_file}.\")\n",
        "        return\n",
        "    merged_segments = merge_segments(utterances, max_duration=combine_duration, max_gap=combine_gap)\n",
        "\n",
        "    if merged_segments:\n",
        "        return merged_segments\n",
        "    else:\n",
        "        return\n",
        "\n",
        "def transcribe_audio_segment(start_time_sec, end_time_sec):\n",
        "    # Convert seconds to milliseconds for pydub\n",
        "    start_time_ms = start_time_sec * 1000\n",
        "    end_time_ms = end_time_sec * 1000\n",
        "\n",
        "    # Extract the audio segment\n",
        "    segment = audio[start_time_ms:end_time_ms]\n",
        "\n",
        "    # Export segment to an in-memory BytesIO object\n",
        "    audio_buffer = io.BytesIO()\n",
        "    segment.export(audio_buffer, format=\"wav\")\n",
        "    audio_buffer.seek(0)  # Reset buffer position to the beginning\n",
        "\n",
        "    files = {\n",
        "        'file': ('audiofile.wav', audio_buffer, 'audio/wav')\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, files=files, data=data)\n",
        "\n",
        "    if response.status_code == 200 or response.status_code == 201:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error for segment {start_time_sec}-{end_time_sec}: {response.status_code} - {response.text}\")\n",
        "        return None\n",
        "\n",
        "def write_srt_file(results, output_file_path):\n",
        "    \"\"\"\n",
        "    Writes the transcription results into an SRT file.\n",
        "\n",
        "    Args:\n",
        "        results (list): List of dictionaries containing 'start_time', 'end_time', and 'transcript'.\n",
        "        output_file_path (str): Path to save the SRT file.\n",
        "    \"\"\"\n",
        "    def format_timestamp(seconds):\n",
        "        \"\"\"Converts seconds to SRT timestamp format: hh:mm:ss,ms\"\"\"\n",
        "        milliseconds = int((seconds % 1) * 1000)\n",
        "        seconds = int(seconds)\n",
        "        minutes = seconds // 60\n",
        "        hours = minutes // 60\n",
        "        seconds = seconds % 60\n",
        "        minutes = minutes % 60\n",
        "        return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
        "\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
        "        for i, result in enumerate(results, start=1):\n",
        "            start_timestamp = format_timestamp(result[\"start_time\"])\n",
        "            end_timestamp = format_timestamp(result[\"end_time\"])\n",
        "            transcript = result[\"transcript\"]\n",
        "\n",
        "            # Write the SRT entry\n",
        "            srt_file.write(f\"{i}\\n\")\n",
        "            srt_file.write(f\"{start_timestamp} --> {end_timestamp}\\n\")\n",
        "            srt_file.write(f\"{transcript}\\n\\n\")\n",
        "\n",
        "api_url = \"https://api.sarvam.ai/speech-to-text-translate\"\n",
        "headers = {\n",
        "    \"api-subscription-key\" : \"d75d7bf3-b053-4084-ac80-c37561a35bfc\"\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"saaras:v2\",\n",
        "}\n",
        "audio_file_path = \"test.wav\"\n",
        "\n",
        "timestamps = process_audio(audio_file_path)\n",
        "audio = AudioSegment.from_file(audio_file_path)\n",
        "\n",
        "results = []\n",
        "\n",
        "for start, end in timestamps:\n",
        "    transcription = transcribe_audio_segment(start, end)\n",
        "    if transcription is not None:\n",
        "        results.append({\n",
        "            \"start_time\": start,\n",
        "            \"end_time\": end,\n",
        "            \"transcript\": transcription[\"transcript\"]\n",
        "        })\n",
        "\n",
        "# Example usage\n",
        "output_srt_path = \"subtitles.srt\"\n",
        "write_srt_file(results, output_srt_path)\n",
        "\n",
        "print(f\"SRT file has been saved to {output_srt_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yTl30sXzqXjv"
      },
      "id": "yTl30sXzqXjv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}