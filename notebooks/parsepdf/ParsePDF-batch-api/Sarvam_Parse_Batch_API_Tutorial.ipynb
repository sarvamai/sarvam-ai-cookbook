{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hY-VHVDJajj7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Sarvam Parse PDF Batch API Tutorial**\n",
        "\n",
        "This notebook demonstrates how to use the **Sarvam PARSE PDF Batch API** to process and extract structured data from multiple PDF files simultaneously. The Batch API allows you to upload a batch of PDFs, parse them, and retrieve their content in HTML format for further analysis.\n",
        "\n",
        "### **Instructions:**\n",
        "\n",
        "1. **Get Your Subscription Key:**\n",
        "   - Go to [dashboard.sarvam.ai](https://dashboard.sarvam.ai) and sign in to your account.\n",
        "   - Copy your **API Subscription Key** for authentication.\n",
        "\n",
        "2. **Set Up Google Colab Folders:**\n",
        "   - Create two folders in your Google Colab notebook:\n",
        "     - **`input/`**: This is where you'll upload your PDF files for batch processing.\n",
        "     - **`output/`**: The parsed files will be saved here, and you can download them after processing.\n",
        "\n",
        "3. **Upload Multiple PDF Files for Batch Processing:**\n",
        "   - Use the **Upload** button in Google Colab to upload all the PDF files you want to process into the **input/** folder.\n",
        "   - Make sure you upload your PDFs as a batch.\n",
        "\n",
        "4. **Run the Script to Parse the PDFs:**\n",
        "   - The script will send the entire batch of uploaded PDF files to the **Sarvam Parse PDF Batch API**, process them, and save the parsed content in **HTML format** in the **output/** folder.\n",
        "\n",
        "5. **Download Processed Files:**\n",
        "   - Once the batch processing is completed, the parsed files will be saved in the **output/** folder. You can then download the processed HTML files from there.\n",
        "\n"
      ],
      "metadata": {
        "id": "hY-VHVDJajj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-storage-file-datalake aiofiles requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMJMTAfkax-4",
        "outputId": "10e4c01b-508d-474f-fcc8-21ded92240d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-storage-file-datalake in /usr/local/lib/python3.11/dist-packages (12.18.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (24.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-file-datalake) (1.32.0)\n",
            "Requirement already satisfied: azure-storage-blob>=12.24.1 in /usr/local/lib/python3.11/dist-packages (from azure-storage-file-datalake) (12.24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-file-datalake) (4.12.2)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-storage-file-datalake) (0.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (1.17.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob>=12.24.1->azure-storage-file-datalake) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.24.1->azure-storage-file-datalake) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set your subscription key here\n",
        "os.environ['SUBSCRIPTION_KEY'] = 'd75d7bf3-b053-4084-ac80-c37561a35bfc'\n",
        "\n",
        "# Verify the environment variable\n",
        "print(\"SUBSCRIPTION_KEY:\", os.getenv('SUBSCRIPTION_KEY'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdVHmnjJfK4Z",
        "outputId": "310000d5-540d-4ff3-caae-e82f5ce37020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUBSCRIPTION_KEY: d75d7bf3-b053-4084-ac80-c37561a35bfc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Libraries**"
      ],
      "metadata": {
        "id": "rvky7VRDa1QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from urllib.parse import urlparse\n",
        "import aiofiles\n",
        "from azure.storage.filedatalake.aio import DataLakeDirectoryClient, FileSystemClient\n",
        "from azure.storage.filedatalake import ContentSettings\n",
        "import mimetypes\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "62y-A_MDa5Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Initialize SarvamJobHandler**\n",
        "\n",
        "The `SarvamJobHandler` class handles job initialization, starting, and status monitoring."
      ],
      "metadata": {
        "id": "vmDedYRca5j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SarvamJobHandler:\n",
        "    BASE_URL = \"https://api.sarvam.ai/parse\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the SarvamJobHandler with the given subscription key.\n",
        "        \"\"\"\n",
        "        self.subscription_key = os.getenv('SUBSCRIPTION_KEY')\n",
        "        if not self.subscription_key:\n",
        "            raise EnvironmentError(\"Environment variable 'SUBSCRIPTION_KEY' is not set.\")\n",
        "\n",
        "    def initialise_job(self):\n",
        "        \"\"\"\n",
        "        Initializes a new job by sending a POST request.\n",
        "        Returns:\n",
        "            dict: Response data containing job details.\n",
        "        \"\"\"\n",
        "        INIT_URL = f\"{self.BASE_URL}/job/init\"\n",
        "        headers = {\n",
        "            'API-Subscription-Key': self.subscription_key\n",
        "        }\n",
        "        try:\n",
        "            response = requests.request(\"POST\", INIT_URL, headers=headers)\n",
        "            response_data = response.json()\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred while trying to initialize job: {e}\")\n",
        "            exit()\n",
        "\n",
        "        return response_data\n",
        "\n",
        "    def start_job(self, job_id, file_details):\n",
        "        \"\"\"\n",
        "        Starts a job with the given parameters by sending a POST request.\n",
        "        Args:\n",
        "            job_id (str): ID of the job.\n",
        "            file_details list[dict]: Dictionary with local file path and corresponding start_page and end_page.\n",
        "\n",
        "        Returns:\n",
        "            dict: Response data from the server.\n",
        "        \"\"\"\n",
        "        START_JOB_URL = f\"{self.BASE_URL}/job\"\n",
        "        headers = {\n",
        "            'API-Subscription-Key': self.subscription_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"job_id\": job_id,\n",
        "            \"job_parameters\": {\n",
        "                \"file_intervals\": file_details,\n",
        "                \"receiver_email\": \"\",\n",
        "                \"sarvam_mode\": \"large\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.request(\"POST\", START_JOB_URL, headers=headers, json=payload)\n",
        "            response_data = response.json()\n",
        "            print(\"Job started successfully:\", json.dumps(response_data, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred while trying to start job: {e}\")\n",
        "            response_data = {}\n",
        "\n",
        "        return response_data\n",
        "\n",
        "    def get_job_status(self, job_id, polling_interval=45):\n",
        "        \"\"\"\n",
        "        Periodically checks the status of a job until it is completed.\n",
        "        Args:\n",
        "            job_id (str): ID of the job to check.\n",
        "            polling_interval (int): Interval between status checks (default is 45 seconds).\n",
        "\n",
        "        Returns:\n",
        "            dict: Final response data when the job is completed.\n",
        "        \"\"\"\n",
        "        headers = {\n",
        "            'API-Subscription-Key': self.subscription_key\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            url = f\"{self.BASE_URL}/job/{job_id}/status\"\n",
        "            try:\n",
        "                response = requests.request(\"GET\", url, headers=headers)\n",
        "                response_data = response.json()\n",
        "\n",
        "                job_state = response_data.get('job_state')\n",
        "                print(f\"Current Job State: {job_state}\")\n",
        "\n",
        "                if job_state == 'Completed':\n",
        "                    print(\"Job has been completed.\")\n",
        "                    return response_data\n",
        "                elif job_state == 'Failed':\n",
        "                    print(f\"Job failed\")\n",
        "                    return response_data\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred while checking job status: {e}\")\n",
        "\n",
        "            time.sleep(polling_interval)"
      ],
      "metadata": {
        "id": "BD3vv1lEapKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Initialize SarvamClient**\n",
        "\n",
        "The `SarvamClient` class handles file uploads, downloads, and deletions from Azure Data Lake Storage.\n"
      ],
      "metadata": {
        "id": "kQZH8yfLbMBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SarvamClient:\n",
        "    def __init__(self, url: str):\n",
        "        # Extract components from the provided URL\n",
        "        self.account_url, self.file_system_name, self.directory_name, self.sas_token = (\n",
        "            self._extract_url_components(url)\n",
        "        )\n",
        "        self.lock = asyncio.Lock()\n",
        "\n",
        "    def update_url(self, url: str):\n",
        "        self.account_url, self.file_system_name, self.directory_name, self.sas_token = (\n",
        "            self._extract_url_components(url)\n",
        "        )\n",
        "\n",
        "    def _extract_url_components(self, url: str):\n",
        "        \"\"\"\n",
        "        Extracts the components from the Azure Data Lake URL.\n",
        "        \"\"\"\n",
        "        # Parse the URL\n",
        "        parsed_url = urlparse(url)\n",
        "\n",
        "        # Construct the account URL and replace blob with dfs for the Data Lake URL\n",
        "        account_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\".replace(\n",
        "            \".blob.\", \".dfs.\"\n",
        "        )\n",
        "\n",
        "        # Split the path to get the file system and directory\n",
        "        path_components = parsed_url.path.strip(\"/\").split(\"/\")\n",
        "\n",
        "        # First part is the file system (e.g., 'bulk-upload-storage')\n",
        "        file_system_name = path_components[0]\n",
        "\n",
        "        # The rest forms the directory path (e.g., 'jobs/swiggy-call-analytics/.../outputs')\n",
        "        directory_name = \"/\".join(path_components[1:])\n",
        "\n",
        "        # Extract the SAS token from the URL query\n",
        "        sas_token = parsed_url.query\n",
        "\n",
        "        return account_url, file_system_name, directory_name, sas_token\n",
        "\n",
        "    async def upload_files(self, local_file_paths, overwrite=True):\n",
        "        \"\"\"\n",
        "        Upload multiple files to the directory extracted from the URL.\n",
        "        \"\"\"\n",
        "        async with DataLakeDirectoryClient(\n",
        "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
        "            file_system_name=self.file_system_name,\n",
        "            directory_name=self.directory_name,\n",
        "            credential=None,\n",
        "        ) as directory_client:\n",
        "            tasks = []\n",
        "            for local_file_path in local_file_paths:\n",
        "                file_name = local_file_path.split(\"/\")[\n",
        "                    -1\n",
        "                ]  # Use the file name from the local path\n",
        "                tasks.append(\n",
        "                    self._upload_file(\n",
        "                        directory_client, local_file_path, file_name, overwrite\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    async def _upload_file(\n",
        "        self, directory_client, local_file_path, file_name, overwrite=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Helper method to upload a single file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            async with aiofiles.open(local_file_path, mode=\"rb\") as file_data:\n",
        "                mime_type, _ = mimetypes.guess_type(local_file_path)\n",
        "                if mime_type is None:\n",
        "                    mime_type = \"audio/wav\"\n",
        "                file_client = directory_client.get_file_client(file_name)\n",
        "                await file_client.upload_data(\n",
        "                    file_data,\n",
        "                    overwrite=overwrite,\n",
        "                    content_settings=ContentSettings(content_type=mime_type),\n",
        "                )\n",
        "                print(f\"File '{file_name}' uploaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to upload '{file_name}': {e}\")\n",
        "\n",
        "    async def list_files(self):\n",
        "        \"\"\"\n",
        "        Return a list of file names (not full paths) in the directory extracted from the URL, protected with a lock.\n",
        "        \"\"\"\n",
        "        file_names = []\n",
        "        async with FileSystemClient(\n",
        "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
        "            file_system_name=self.file_system_name,\n",
        "            credential=None,\n",
        "        ) as file_system_client:\n",
        "            async for path in file_system_client.get_paths(self.directory_name):\n",
        "                file_name = path.name.split(\"/\")[\n",
        "                    -1\n",
        "                ]  # Extract the last part of the path (file name)\n",
        "                async with self.lock:  # Acquire lock before modifying file_names\n",
        "                    file_names.append(file_name)\n",
        "        return file_names\n",
        "\n",
        "    async def download_files(self, file_names, destination_dir):\n",
        "        \"\"\"\n",
        "        Download files from the directory extracted from the URL to a local directory.\n",
        "        \"\"\"\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "        destination_dir.rsplit(\"/\")[-1]\n",
        "        async with DataLakeDirectoryClient(\n",
        "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
        "            file_system_name=self.file_system_name,\n",
        "            directory_name=self.directory_name,\n",
        "            credential=None,\n",
        "        ) as directory_client:\n",
        "            tasks = []\n",
        "            for file_name in file_names:\n",
        "                tasks.append(\n",
        "                    self._download_file(directory_client, file_name, destination_dir)\n",
        "                )\n",
        "\n",
        "            await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    async def _download_file(self, directory_client, file_name, destination_dir):\n",
        "        \"\"\"\n",
        "        Helper method to download a single file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_client = directory_client.get_file_client(file_name)\n",
        "            download_path = f\"{destination_dir}/{file_name}\"\n",
        "            async with aiofiles.open(download_path, mode=\"wb\") as file_data:\n",
        "                stream = await file_client.download_file()\n",
        "                data = await stream.readall()\n",
        "                await file_data.write(data)\n",
        "            print(f\"File '{file_name}' downloaded successfully to '{download_path}'!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download '{file_name}': {e}\")\n",
        "\n",
        "    async def delete_files(self, file_names):\n",
        "        \"\"\"\n",
        "        Delete files from the directory extracted from the URL.\n",
        "        \"\"\"\n",
        "        async with DataLakeDirectoryClient(\n",
        "            account_url=f\"{self.account_url}?{self.sas_token}\",\n",
        "            file_system_name=self.file_system_name,\n",
        "            directory_name=self.directory_name,\n",
        "            credential=None,\n",
        "        ) as directory_client:\n",
        "            tasks = []\n",
        "            for file_name in file_names:\n",
        "                tasks.append(self._delete_file(directory_client, file_name))\n",
        "\n",
        "            await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    async def _delete_file(self, directory_client, file_name):\n",
        "        \"\"\"\n",
        "        Helper method to delete a single file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_client = directory_client.get_file_client(file_name)\n",
        "            await file_client.delete_file()\n",
        "            print(f\"File '{file_name}' deleted successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete '{file_name}': {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E_ztvDembXzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Step 3: Define Helper Functions**\n",
        "\n",
        "### **Get File Details**\n",
        "\n",
        "This function collects file details (file name, start page, and end page) from the user.\n"
      ],
      "metadata": {
        "id": "8qJNuRVBbbGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_details():\n",
        "    \"\"\"\n",
        "    Collects file details (file name, start page, and end page) from the user.\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing file details.\n",
        "    \"\"\"\n",
        "    file_paths = []\n",
        "    file_details = []\n",
        "    print(\"Enter file details. Type 'done' when you are finished.\\n\")\n",
        "    while True:\n",
        "        file_name = input(\"Enter file path (or type 'done' to finish): \").strip()\n",
        "        if file_name.lower() == 'done':\n",
        "            break\n",
        "        try:\n",
        "            start_page = int(input(f\"Enter start page for '{file_name}': \").strip())\n",
        "            end_page = int(input(f\"Enter end page for '{file_name}': \").strip())\n",
        "\n",
        "            if start_page > end_page:\n",
        "                print(\"Error: Start page cannot be greater than end page. Please try again.\")\n",
        "                continue\n",
        "            file_paths.append(file_name)\n",
        "            file_details.append({\n",
        "                \"file_name\": file_name.split(\"/\")[-1],\n",
        "                \"page_intervals\": [\n",
        "                    {\n",
        "                        \"start_page\": start_page,\n",
        "                        \"end_page\": end_page\n",
        "                    }\n",
        "                ]\n",
        "            })\n",
        "        except ValueError:\n",
        "            print(\"Invalid input! Please enter numeric values for start and end pages.\")\n",
        "\n",
        "    return file_paths, file_details"
      ],
      "metadata": {
        "id": "7aVJ9oAhbhut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convert JSON to HTML**\n",
        "\n",
        "This function converts JSON output files to HTML."
      ],
      "metadata": {
        "id": "sCwAtWiVcAqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_html(input_dir, output_dir):\n",
        "    import base64\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for filename in sorted(os.listdir(input_dir)):\n",
        "        if filename.endswith('.json'):\n",
        "            file_number = filename.split('.')[0]\n",
        "            with open(os.path.join(input_dir, filename), 'r') as json_file:\n",
        "                data = json.load(json_file)\n",
        "            html_content = base64.b64decode(data['output']).decode('utf-8')\n",
        "            output_filename = f'{file_number}.html'\n",
        "            with open(os.path.join(output_dir, output_filename), 'w', encoding='utf-8') as html_file:\n",
        "                html_file.write(html_content)\n",
        "\n",
        "    print(f\"Converted {len(os.listdir(input_dir))} JSON files to HTML in {output_dir}\")\n",
        "    return\n",
        "\n"
      ],
      "metadata": {
        "id": "nxkvng8zcE8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Run the Main Function**\n",
        "\n",
        "This function orchestrates the entire process:\n",
        "1. Initializes the job.\n",
        "2. Uploads files.\n",
        "3. Starts the job.\n",
        "4. Monitors the job status.\n",
        "5. Downloads and converts output files."
      ],
      "metadata": {
        "id": "O9dPGHdvblRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # Initialise job\n",
        "    sarvam_handler = SarvamJobHandler()\n",
        "    job_data = sarvam_handler.initialise_job()\n",
        "\n",
        "    job_id = job_data.get('job_id')\n",
        "    print(f\"Extracted job id: {job_id}\")\n",
        "    if job_id is None:\n",
        "        exit()\n",
        "\n",
        "    # Get file paths and page intervals\n",
        "    local_file_paths, file_details = get_file_details()\n",
        "    # Upload files to storage\n",
        "    client = SarvamClient(job_data.get('input_storage_path'))\n",
        "    await client.upload_files(local_file_paths, overwrite=True)\n",
        "    print(await client.list_files())\n",
        "\n",
        "    # Start the job\n",
        "    sarvam_handler.start_job(job_id, file_details)\n",
        "\n",
        "    # Check job status\n",
        "    final_status = sarvam_handler.get_job_status(job_id)\n",
        "    print(\"Final Job Status:\", json.dumps(final_status, indent=2))\n",
        "\n",
        "    if final_status.get('job_state') == 'Completed':\n",
        "        # Download the jsons\n",
        "        client.update_url(url=job_data.get('output_storage_path'))\n",
        "        files = await client.list_files()\n",
        "        await client.download_files(file_names=files, destination_dir=\"./data\")\n",
        "\n",
        "        # Convert downloaded files to HTML\n",
        "        input_dir = './data'\n",
        "        output_dir = './output'\n",
        "        convert_to_html(input_dir, output_dir)\n",
        "\n",
        "# Run the example\n",
        "await(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM5v1ERYb78l",
        "outputId": "67c25e4f-5a11-448e-c562-fc79965e96c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted job id: 20250225_6f46fe5d-24f5-4668-becf-1c1c8affafd7\n",
            "Enter file details. Type 'done' when you are finished.\n",
            "\n",
            "Enter file path (or type 'done' to finish): /content/input/temp.pdf\n",
            "Enter start page for '/content/input/temp.pdf': 1\n",
            "Enter end page for '/content/input/temp.pdf': 1\n",
            "Enter file path (or type 'done' to finish): /content/input/temp1.pdf\n",
            "Enter start page for '/content/input/temp1.pdf': 1\n",
            "Enter end page for '/content/input/temp1.pdf': 1\n",
            "Enter file path (or type 'done' to finish): done\n",
            "File 'temp.pdf' uploaded successfully!\n",
            "File 'temp1.pdf' uploaded successfully!\n",
            "['temp.pdf', 'temp1.pdf']\n",
            "Job started successfully: {\n",
            "  \"job_status\": {\n",
            "    \"job_state\": \"Pending\",\n",
            "    \"created_at\": \"2025-02-25T14:55:11.581107+00:00\",\n",
            "    \"updated_at\": \"2025-02-25T14:55:35.746629+00:00\",\n",
            "    \"job_id\": \"20250225_6f46fe5d-24f5-4668-becf-1c1c8affafd7\",\n",
            "    \"total_files\": 0,\n",
            "    \"successful_files_count\": 0,\n",
            "    \"failed_files_count\": 0,\n",
            "    \"owner_id\": \"ba3a72b2144ea99c97e390e5f0f43780c693171b6fd7f68ad7dde2f62dcb801a\",\n",
            "    \"input_storage_path\": \"https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-25/PARSE_PDF_BULK/6f46fe5d-24f5-4668-becf-1c1c8affafd7/inputs?se=2025-02-25T15%3A55%3A11Z&sp=wdl&sv=2025-01-05&sr=d&sdd=5&sig=JIoRFEOAqGrrmkzUlg3UtQa3yboTzXHkxqF0ORU505c%3D\",\n",
            "    \"output_storage_path\": \"https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-25/PARSE_PDF_BULK/6f46fe5d-24f5-4668-becf-1c1c8affafd7/outputs?se=2025-03-04T14%3A55%3A11Z&sp=rl&sv=2025-01-05&sr=d&sdd=5&sig=LGQPAy2f%2BDH5QAs18nOkQbhgfAFpH8ruPvfg3HmCw7Y%3D\",\n",
            "    \"storage_container_type\": \"Azure\",\n",
            "    \"error_message\": \"\",\n",
            "    \"job_details\": []\n",
            "  }\n",
            "}\n",
            "Current Job State: Running\n",
            "Current Job State: Completed\n",
            "Job has been completed.\n",
            "Final Job Status: {\n",
            "  \"job_state\": \"Completed\",\n",
            "  \"created_at\": \"2025-02-25T14:55:11.581107+00:00\",\n",
            "  \"updated_at\": \"2025-02-25T14:56:16.550799+00:00\",\n",
            "  \"job_id\": \"20250225_6f46fe5d-24f5-4668-becf-1c1c8affafd7\",\n",
            "  \"total_files\": 2,\n",
            "  \"successful_files_count\": 2,\n",
            "  \"failed_files_count\": 0,\n",
            "  \"owner_id\": \"ba3a72b2144ea99c97e390e5f0f43780c693171b6fd7f68ad7dde2f62dcb801a\",\n",
            "  \"input_storage_path\": \"https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-25/PARSE_PDF_BULK/6f46fe5d-24f5-4668-becf-1c1c8affafd7/inputs?se=2025-02-25T15%3A55%3A11Z&sp=wdl&sv=2025-01-05&sr=d&sdd=5&sig=JIoRFEOAqGrrmkzUlg3UtQa3yboTzXHkxqF0ORU505c%3D\",\n",
            "  \"output_storage_path\": \"https://appsprodbulkjobssa.blob.core.windows.net/bulk-upload-storage/jobs/2025-02-25/PARSE_PDF_BULK/6f46fe5d-24f5-4668-becf-1c1c8affafd7/outputs?se=2025-03-04T14%3A55%3A11Z&sp=rl&sv=2025-01-05&sr=d&sdd=5&sig=LGQPAy2f%2BDH5QAs18nOkQbhgfAFpH8ruPvfg3HmCw7Y%3D\",\n",
            "  \"storage_container_type\": \"Azure\",\n",
            "  \"error_message\": \"\",\n",
            "  \"job_details\": [\n",
            "    {\n",
            "      \"file_name\": \"temp.pdf\",\n",
            "      \"file_id\": \"0-1\",\n",
            "      \"state\": \"Success\",\n",
            "      \"error_message\": \"\"\n",
            "    },\n",
            "    {\n",
            "      \"file_name\": \"temp1.pdf\",\n",
            "      \"file_id\": \"1-1\",\n",
            "      \"state\": \"Success\",\n",
            "      \"error_message\": \"\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "File '1-1.json' downloaded successfully to './data/1-1.json'!\n",
            "File '0-1.json' downloaded successfully to './data/0-1.json'!\n",
            "Converted 2 JSON files to HTML in ./output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Conclusion**\n",
        "\n",
        "This notebook demonstrated how to use the **Sarvam PDF Parser API** to extract structured data from PDF files. By following the steps, you can:\n",
        "\n",
        "1. Upload a PDF file.\n",
        "2. Parse the file using the Sarvam API.\n",
        "3. Download the parsed HTML content for further analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Additional Resources**\n",
        "\n",
        "For more details, refer to the official **Sarvam API documentation** and join the community for support:\n",
        "\n",
        "- **Documentation**: [docs.sarvam.ai](https://docs.sarvam.ai)  \n",
        "- **Community**: [Join the Discord Community](https://discord.gg/hTuVuPNF)\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Final Notes**\n",
        "\n",
        "- Keep your API key secure.\n",
        "- Explore advanced features like multi-page parsing and custom output formats.\n",
        "\n",
        "Happy parsing! ðŸš€\n"
      ],
      "metadata": {
        "id": "SihjP-xWgjXl"
      }
    }
  ]
}